{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso seja necessária a instalação de bibliotecas\n",
    "# #pip install sty bibtexparser pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas necessárias\n",
    "import bibtexparser\n",
    "from bibtexparser.bwriter import BibTexWriter\n",
    "from bibtexparser.bibdatabase import BibDatabase\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrutura de controle\n",
    "chaves=list()\n",
    "writer = BibTexWriter()\n",
    "writer.indent = ' '     # indent entries with 4 spaces instead of one\n",
    "writer.comma_first = False  # place the comma at the beginning of the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções\n",
    "def parser_bibtex(file_bib):\n",
    "    with open(file_bib,encoding='utf-8') as bibtex_file:\n",
    "        try:\n",
    "            bib_database = bibtexparser.load(bibtex_file)\n",
    "            return pd.DataFrame(bib_database.entries)\n",
    "        except:\n",
    "            print('Falha no arquivo', file_bib)\n",
    "            return pd.DataFrame(None)\n",
    "def drop_invalid(df_tmp):\n",
    "    for x in range(len(df_tmp)-1,-1,-1):\n",
    "        if((df_tmp.loc[x].loc['ID'])[:8] == 'NoAuthor'):\n",
    "            df_tmp.drop([int(x)], inplace=True)\n",
    "        elif(df_tmp.loc[x].loc['author'] == ''):\n",
    "            df_tmp.drop([int(x)], inplace=True)\n",
    "    df_tmp.reset_index(inplace=True,drop=True)\n",
    "    return df_tmp,len(df_tmp)\n",
    "    \n",
    "def drop_not_doi(df_tmp):\n",
    "    for x in range(len(df_tmp)-1,-1,-1):\n",
    "        if((df_tmp.loc[x].loc['doi']) == ''):\n",
    "            df_tmp.drop([int(x)], inplace=True)\n",
    "    df_tmp.reset_index(inplace=True,drop=True)\n",
    "    return df_tmp,len(df_tmp)\n",
    "\n",
    "def drop_year(df_tmp,ano):\n",
    "    for x in range(len(df_tmp)-1,-1,-1):\n",
    "        if(int(df_tmp.loc[x].loc['year']) < ano):\n",
    "            df_tmp.drop([int(x)], inplace=True)\n",
    "    df_tmp.reset_index(inplace=True,drop=True)\n",
    "    return df_tmp,len(df_tmp)\n",
    "\n",
    "def drop_pag(df_tmp,pag):\n",
    "    for x in range(len(df_tmp)-1,-1,-1):\n",
    "        #print(df_tmp.loc[x].loc['pages'])\n",
    "        z = (str(df_tmp.loc[x].loc['pages'])).split('-')\n",
    "        if(len(z) == 4):\n",
    "            w=int(z[3])-int(z[1])\n",
    "        elif(len(z) == 2):\n",
    "            if((z[1].isnumeric())and(z[0].isnumeric())):\n",
    "                w=int(z[1])-int(z[0])\n",
    "            else:\n",
    "                w=0\n",
    "        else:\n",
    "            w=pag+1\n",
    "        if(w < 4):\n",
    "            df_tmp.drop([int(x)], inplace=True)\n",
    "    df_tmp.reset_index(inplace=True,drop=True)\n",
    "    return df_tmp,len(df_tmp)\n",
    "\n",
    "def drop_source(df_tmp,entrada,tipo):\n",
    "    for x in range(len(df_tmp)-1,-1,-1):\n",
    "        try:\n",
    "            if(df_tmp.loc[x].loc['ENTRYTYPE'] in entrada):\n",
    "                df_tmp.drop([int(x)], inplace=True)\n",
    "            elif(df_tmp.loc[x].loc['document_type'] in tipo):\n",
    "                df_tmp.drop([int(x)], inplace=True)\n",
    "        except:\n",
    "            None\n",
    "    df_tmp.reset_index(inplace=True,drop=True)\n",
    "    return df_tmp,len(df_tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definidas as chaves de busca:\n",
    "\n",
    "1. critical infrastructure AND (vulnerabilit* AND (analys* OR Assessment OR characterization OR scan* OR tool)) AND (threat AND (shar* OR intelligent))\n",
    "1. network AND (Security OR ``Vulnerabilit* Tool'' OR scan*) AND shar* AND (threat OR risk OR intelligent OR information) AND risk AND (Classification OR reduction OR mitigation)\n",
    "1. threat AND (shar* OR intelligent) AND test AND (intrusion OR penetration)\n",
    "1. cyber AND security AND assessment AND threat AND vulnerabilit*\n",
    "\n",
    "Extraiu-se as referências dos resultados da busca (Export) em arquivos [base]-[chave]_[página].bib\n",
    "\n",
    "Aplicação dos Critérios de Elegibilidade, que podem ser avaliados objetivamente com os dados da referência.\n",
    "\n",
    "Aplicação de demais Critérios e Seleção de Estudos:\n",
    "-  Utiliza Inteligência de Ameaças?\n",
    "-  Identifica ou avalia vulnerabilidades?\n",
    "-  Utiliza orquestração de ferramentas?\n",
    "-  Utiliza automatização de processos? \n",
    "-  Mitiga vulnerabilidades em redes? \n",
    "-  Atua com medidas preventivas em Segurança Cibernética?\n",
    "-  Possui relacionamento com Infraestrutura Crítica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerar Dataframe dos arquivos gerados nas Bases de busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar arquivos\n",
    "y=0\n",
    "for x in os.listdir():\n",
    "    if('.bib' in x):\n",
    "        chaves.append(['df_' + x.split(\".\")[0].replace('-','_')])\n",
    "        globals()[chaves[y][0]] = parser_bibtex(x)\n",
    "        chaves[y].append(len(globals()[chaves[y][0]]))\n",
    "        y+=1\n",
    "del x,y\n",
    "chaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicar Critérios de Elegibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrada Inválida\n",
    "for y,chave in enumerate(chaves):\n",
    "    globals()[chave[0]],aux = drop_invalid(globals()[chave[0]])\n",
    "    chaves[y].append(aux)\n",
    "del aux,y\n",
    "chaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ano\n",
    "ano=2010\n",
    "for y,chave in enumerate(chaves):\n",
    "    globals()[chave[0]],aux = drop_year(globals()[chave[0]],ano)\n",
    "    chaves[y].append(aux)\n",
    "del aux,y\n",
    "chaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de páginas\n",
    "pag = 3\n",
    "for y,chave in enumerate(chaves):\n",
    "    globals()[chave[0]],aux = drop_pag(globals()[chave[0]],pag)\n",
    "    chaves[y].append(aux)\n",
    "del aux,y\n",
    "chaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não indexado\n",
    "for y,chave in enumerate(chaves):\n",
    "    globals()[chave[0]],aux = drop_not_doi(globals()[chave[0]])\n",
    "    chaves[y].append(aux)\n",
    "del aux,y\n",
    "chaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonte\n",
    "entrada = ['book','inbook']\n",
    "tipo = ['Book Chapter','Article in Press','Note']\n",
    "for y,chave in enumerate(chaves):\n",
    "    globals()[chave[0]],aux = drop_source(globals()[chave[0]],entrada,tipo)\n",
    "    chaves[y].append(aux)\n",
    "del aux,y\n",
    "chaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar por Base de Conhecimento e Remover Duplicatas\n",
    "df_ieee = pd.DataFrame(None)\n",
    "df_scopus = pd.DataFrame(None)\n",
    "for y,chave in enumerate(chaves):\n",
    "    if('ieee' in chave[0]):\n",
    "        df_ieee = pd.concat([df_ieee,globals()[chave[0]]], ignore_index=True)\n",
    "    elif(('scopus')in chave[0]):\n",
    "        df_scopus = pd.concat([df_scopus,globals()[chave[0]]], ignore_index=True)\n",
    "    del globals()[chave[0]]\n",
    "bases = [['df_ieee',len(df_ieee)],['df_scopus',len(df_scopus)]]\n",
    "for y,base in enumerate(bases):\n",
    "    globals()[base[0]].drop_duplicates(subset=['ID','title'],inplace=True)\n",
    "    globals()[base[0]].reset_index(inplace=True,drop=True)\n",
    "    bases[y].append(len(globals()[base[0]]))\n",
    "    \n",
    "del y\n",
    "bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar por Base de Conhecimento e Remover Duplicatas\n",
    "df = pd.concat([df_ieee,df_scopus], ignore_index=True)\n",
    "pesquisa=['dados',len(df)]\n",
    "df.drop_duplicates(subset=['doi','title'],inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "pesquisa.append(len(df))\n",
    "\n",
    "del df_ieee,df_scopus\n",
    "pesquisa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relatório de aplicação [em construção]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerar arquivo de referênicas .bib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['abstract','keywords'],axis=1)\n",
    "df = df.replace(np.nan, '', regex=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df = df.to_dict(orient=\"records\")\n",
    "db = BibDatabase()\n",
    "db.entries = df\n",
    "\n",
    "with open('referencias_mapeamento.bib', 'a',encoding='utf-8') as bibfile:\n",
    "    bibfile.write(writer.write(db))\n",
    "    \n",
    "del df,db\n",
    "del writer, bibfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
